import{_ as e,c as t,o as n,ag as l}from"./chunks/framework.ByciF0Oj.js";const s=JSON.parse('{"title":"LangChat如何接入DeepSeek-R1模型","description":"","frontmatter":{},"headers":[],"relativePath":"langchat/docs/langchat-deepseek-r1.md","filePath":"langchat/docs/langchat-deepseek-r1.md","lastUpdated":null}'),o={name:"langchat/docs/langchat-deepseek-r1.md"};function i(c,a,p,g,d,r){return n(),t("div",null,a[0]||(a[0]=[l(`<h1 id="langchat如何接入deepseek-r1模型" tabindex="-1">LangChat如何接入DeepSeek-R1模型 <a class="header-anchor" href="#langchat如何接入deepseek-r1模型" aria-label="Permalink to &quot;LangChat如何接入DeepSeek-R1模型&quot;">​</a></h1><blockquote><p>本教程给使用LangChat的朋友学习如何本地部署DeepSeek-R1模型。以及如何使用LangChat的Agent功能构建知识库。</p></blockquote><h3 id="关于langchat" tabindex="-1">关于LangChat <a class="header-anchor" href="#关于langchat" aria-label="Permalink to &quot;关于LangChat&quot;">​</a></h3><p><strong>LangChat</strong> 是Java生态下企业级AIGC项目解决方案，集成RBAC和AIGC大模型能力，帮助企业快速定制AI知识库、企业AI机器人。</p><p><strong>支持的AI大模型：</strong> Gitee AI / 阿里通义 / 百度千帆 / DeepSeek / 抖音豆包 / 智谱清言 / 零一万物 / 讯飞星火 / OpenAI / Gemini / Ollama / Azure / Claude 等大模型。</p><ul><li>官网地址：<a href="http://langchat.cn/" target="_blank" rel="noreferrer">http://langchat.cn/</a></li></ul><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li></ul><h2 id="安装deepseek-r1" tabindex="-1">安装DeepSeek-R1 <a class="header-anchor" href="#安装deepseek-r1" aria-label="Permalink to &quot;安装DeepSeek-R1&quot;">​</a></h2><p>DeepSeek-R1是由DeepSeek公司推出的开源大模型，目前最强的低成本推理模型。</p><p><strong>注意：</strong> 这里仅介绍使用Ollama部署DeepSeek-R1模型，其他模型的部署方式可以参考官方文档。</p><h3 id="基础概念" tabindex="-1">基础概念 <a class="header-anchor" href="#基础概念" aria-label="Permalink to &quot;基础概念&quot;">​</a></h3><p>首先，必须给大家介绍一些基础的概念，避免大家有各种疑惑。</p><ol><li>Ollama是安装模型最简单的方式，不需要python、Docker以及其他复杂的过程。</li><li>无论本地安装DeepSeek-R1的哪个模型，其实区别都不大，因为都是阉割版的，所以大家不要被营销号带偏。</li><li>Ollama的<code>run</code>命令执行后，首先会检查本地有没有此模型，有就安装，没有就直接运行模型。（Ollama客户端兼容全平台，不是非要Linux服务器）</li><li>Ollama的<code>run</code>启动模型后会暴露 <strong>11434</strong> HTTP端口，其他所有的LLM OPS应用都是通过此端口和模型交互的，你可以访问 <a href="http://127.0.0.1:11434" target="_blank" rel="noreferrer">http://127.0.0.1:11434</a> 查看</li><li>DeepSeek-R1是推理模型，Embedding模型需要单独安装，这两者不是一个概念，如果需要知识库向量化，不应该用DeepSeek-R1</li><li>......</li></ol><h3 id="deepseek-r1下载哪个版本" tabindex="-1">DeepSeek-R1下载哪个版本？ <a class="header-anchor" href="#deepseek-r1下载哪个版本" aria-label="Permalink to &quot;DeepSeek-R1下载哪个版本？&quot;">​</a></h3><p><strong>实际上，本地测试而言，无论你下载哪个版本，最终的效果都是一样的</strong>。因为都是阉割版的小参数模型，所以我推荐各位安装 <strong>1.5B</strong> 或者 <strong>7/8B</strong> 测试即可。真实的场景下，还是推荐调用API。</p><p>Ollama地址：<a href="https://ollama.com/library/deepseek-r1" target="_blank" rel="noreferrer">https://ollama.com/library/deepseek-r1</a></p><p><img src="http://cdn.tycoding.cn/docs/202502111644203.png" alt="image-20250211164354673" loading="lazy"></p><p><strong>注意：</strong></p><p>运行 <code>ollama run deepseek-r1</code> 命令默认安装的 <strong>7B</strong> 版本。</p><ol><li>1.5B 模型，基本上任意笔记本都能安装</li><li>7/8B 模型，至少本地电脑有16G内存</li><li>14B以上模型，至少本地电脑有32G内存</li></ol><p><strong>作者本地电脑是 Macbook m3 16GB + 512版本</strong></p><h3 id="_1-安装ollama" tabindex="-1">1. 安装Ollama <a class="header-anchor" href="#_1-安装ollama" aria-label="Permalink to &quot;1. 安装Ollama&quot;">​</a></h3><p>Ollama官网地址 <a href="https://ollama.com/" target="_blank" rel="noreferrer">https://ollama.com/</a></p><p><img src="http://cdn.tycoding.cn/docs/202502111648168.png" alt="image-20250211164855062" loading="lazy"></p><p>这里会根据你的操作系统下载对应的安装包</p><p><strong>安装Ollama的步骤这里就不再解释了</strong></p><h3 id="_2-安装deepseek-r1" tabindex="-1">2. 安装DeepSeek-R1 <a class="header-anchor" href="#_2-安装deepseek-r1" aria-label="Permalink to &quot;2. 安装DeepSeek-R1&quot;">​</a></h3><blockquote><p>如果你本地电脑是 &gt;=16G内存，就用默认的安装命令，否则建议安装1.5B小模型</p></blockquote><p><img src="http://cdn.tycoding.cn/docs/202502111652139.png" alt="image-20250211165252956" loading="lazy"></p><p>因为作者是16G笔记本，所以这里直接按照默认的下载7B模型</p><p>正常情况下，如上图所示，本地电脑就已经安装好了DeepSeek-R1模型。</p><h3 id="_3-验证deepseek-r1模型是否启动" tabindex="-1">3. 验证DeepSeek-R1模型是否启动 <a class="header-anchor" href="#_3-验证deepseek-r1模型是否启动" aria-label="Permalink to &quot;3. 验证DeepSeek-R1模型是否启动&quot;">​</a></h3><p>如上，你可以在控制台直接交互。</p><p>另外本地访问 <a href="http://127.0.0.1:11434" target="_blank" rel="noreferrer">http://127.0.0.1:11434</a> 地址，能看到如下信息：</p><p><img src="http://cdn.tycoding.cn/docs/202502111702790.png" alt="image-20250211170217736" loading="lazy"></p><h2 id="启动langchat" tabindex="-1">启动LangChat <a class="header-anchor" href="#启动langchat" aria-label="Permalink to &quot;启动LangChat&quot;">​</a></h2><blockquote><p>注意：LangChat至少需要以下环境：</p><ol><li>MySQL8</li><li>JDK17+</li><li>PgVector等</li></ol></blockquote><p><strong>开源地址：</strong></p><ul><li>Gitee：<a href="https://gitee.com/langchat/langchat" target="_blank" rel="noreferrer">https://gitee.com/langchat/langchat</a></li><li>Github：<a href="https://github.com/tycoding/langchat" target="_blank" rel="noreferrer">https://github.com/tycoding/langchat</a></li><li>GitCode: <a href="https://gitcode.com/LangChat/LangChat" target="_blank" rel="noreferrer">https://gitcode.com/LangChat/LangChat</a></li></ul><p>首先本地IDEA打开LangChat项目（等待Maven加载完成）</p><p><img src="http://cdn.tycoding.cn/docs/202502111704697.png" alt="image-20250211170424610" loading="lazy"></p><h3 id="_1-执行数据库脚本" tabindex="-1">1. 执行数据库脚本 <a class="header-anchor" href="#_1-执行数据库脚本" aria-label="Permalink to &quot;1. 执行数据库脚本&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502111705186.png" alt="image-20250211170540137" loading="lazy"></p><p>在docs/目录下找到<code>langchat.sql</code> 在MySQL中执行此脚本。</p><p><strong>注意：</strong> 此脚本包含了创建名为<code>langchat</code>的数据库（因此不需要手动创建数据库）</p><h3 id="_2-修改配置文件" tabindex="-1">2. 修改配置文件 <a class="header-anchor" href="#_2-修改配置文件" aria-label="Permalink to &quot;2. 修改配置文件&quot;">​</a></h3><p>首先你应该检查SpringBoot的<code>application-*.yml</code>配置文件</p><p><img src="http://cdn.tycoding.cn/docs/202502111707940.png" alt="image-20250211170752880" loading="lazy"></p><p><strong>必须修改：</strong></p><ol><li>MySQL连接信息</li><li>OSS信息（默认的<code>local</code>代表了使用tomcat的地址，当然建议使用阿里云或七牛云，或者本地用NGINX搭建本地文件服务器）</li></ol><h3 id="_3-安装pgvector" tabindex="-1">3. 安装PgVector <a class="header-anchor" href="#_3-安装pgvector" aria-label="Permalink to &quot;3. 安装PgVector&quot;">​</a></h3><blockquote><p>这里我只推荐Pgvector，不要用Redis，Pgvector可以用navicat等工具可视化查看数据表</p></blockquote><p>Pgvector官方仓库：<a href="https://github.com/pgvector/pgvector?tab=readme-ov-file#installation-notes---windows" target="_blank" rel="noreferrer">https://github.com/pgvector/pgvector?tab=readme-ov-file#installation-notes---windows</a></p><p>Postgres官网：<a href="https://postgresapp.com/downloads.html" target="_blank" rel="noreferrer">https://postgresapp.com/downloads.html</a></p><p>注意：安装Pgvector后仍需要有Postgres 15+基础环境。所以如果你是第一次安装，你需要安装两者才行。</p><p><img src="http://cdn.tycoding.cn/docs/202502111715986.png" alt="image-20250211171529888" loading="lazy"></p><p>因为我使用的Mac，所以有多种安装方式，如果不想麻烦可以用Docker</p><p><img src="http://cdn.tycoding.cn/docs/202502111716476.png" alt="image-20250211171648413" loading="lazy"></p><p>作者贴心的给大家编译了一个pgvector发布到了阿里云仓库，直接运行此compose也可启动，省去了上麦那一系列步骤</p><p>如果上面脚本执行成功，应该在数据库能看到<code>langchat</code></p><p><img src="http://cdn.tycoding.cn/docs/202502111718850.png" alt="image-20250211171821785" loading="lazy"></p><p><strong>注意：</strong></p><ol><li>如果是自己手动安装的Pgvector，请手动创建<code>langchat</code>数据库</li><li>尽量不要自己手动编译pgvector源码，太麻烦了</li></ol><h3 id="_4-运行langchat" tabindex="-1">4. 运行LangChat <a class="header-anchor" href="#_4-运行langchat" aria-label="Permalink to &quot;4. 运行LangChat&quot;">​</a></h3><p>上述配置完毕后，即可正常启动LangChat</p><p><img src="http://cdn.tycoding.cn/docs/202502111722185.png" alt="image-20250211172222107" loading="lazy"></p><p>启动成功后如上图，注意 <strong>当前环境是什么就代表用了哪个配置文件</strong></p><p><strong>运行langchat-ui</strong></p><p><img src="http://cdn.tycoding.cn/docs/202502111723098.png" alt="image-20250211172348017" loading="lazy"></p><h2 id="测试langchat" tabindex="-1">测试LangChat <a class="header-anchor" href="#测试langchat" aria-label="Permalink to &quot;测试LangChat&quot;">​</a></h2><p>首先进入到LangChat此页面</p><p><img src="http://cdn.tycoding.cn/docs/202502111727377.png" alt="image-20250211172719346" loading="lazy"></p><p>因为我们使用的Ollama部署的DeepSeek-R1模型，因此必须使用Ollama配置</p><ol><li>模型版本写： <code>deepseek-r1</code></li><li>BaseUlr写：<code>http://127.0.0.1:11434/</code></li><li>ApiKey任意填</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502111729516.png" alt="image-20250211172945369" loading="lazy"></p><h3 id="测试langchat聊天功能" tabindex="-1">测试LangChat聊天功能 <a class="header-anchor" href="#测试langchat聊天功能" aria-label="Permalink to &quot;测试LangChat聊天功能&quot;">​</a></h3><blockquote><p>到此为止，DeepSeek-R1模型已经启动并配置好，我们先测试Chat基础功能</p></blockquote><p><img src="http://cdn.tycoding.cn/docs/202502111731492.png" alt="image-20250211173131416" loading="lazy"></p><p><img src="http://cdn.tycoding.cn/docs/202502111733161.png" alt="image-20250211173310009" loading="lazy"></p><p>如上，接口已经掉通了。</p><p><strong>注意： <code>&lt;think&gt;</code>是DeekSeek-R1的推理过程，因为他是非标准的数据格式，后面LangChat会做前端适配</strong></p><h2 id="配置langchat知识库" tabindex="-1">配置LangChat知识库 <a class="header-anchor" href="#配置langchat知识库" aria-label="Permalink to &quot;配置LangChat知识库&quot;">​</a></h2><blockquote><p>首先你需要安装好Pgvector和OSS</p></blockquote><h3 id="_1-配置向量数据库" tabindex="-1">1. 配置向量数据库 <a class="header-anchor" href="#_1-配置向量数据库" aria-label="Permalink to &quot;1. 配置向量数据库&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502111736653.png" alt="image-20250211173652563" loading="lazy"></p><p><strong>注意！注意！注意！</strong></p><p><strong>建议不要修改向量维度这个参数，向量数据表一旦初始化，此表的向量维度就固定了，只能接受指定向量的数据，因此仅在LangChat前端修改是无效的（需要删除原表）</strong></p><h3 id="_2-本地下载embedding模型" tabindex="-1">2. 本地下载Embedding模型 <a class="header-anchor" href="#_2-本地下载embedding模型" aria-label="Permalink to &quot;2. 本地下载Embedding模型&quot;">​</a></h3><blockquote><p>当然我建议大家直接使用阿里云、百度、智谱的Embedding模型，这样只需要配置ApiKey即可，但是很多朋友可能还想想本地部署，这里教大家。</p></blockquote><p>同样，使用Ollama下载Embedding模型</p><p>进入官网：<a href="https://ollama.com/search?c=embedding" target="_blank" rel="noreferrer">https://ollama.com/search?c=embedding</a> 我们找到排名第一的Embedding模型</p><p><img src="http://cdn.tycoding.cn/docs/202502112049972.png" alt="image-20250211204936831" loading="lazy"></p><p>执行命令 <code>ollama pull nomic-embed-text</code></p><p><img src="http://cdn.tycoding.cn/docs/202502112052648.png" alt="image-20250211205200587" loading="lazy"></p><p><img src="http://cdn.tycoding.cn/docs/202502112052670.png" alt="image-20250211205253605" loading="lazy"></p><p>如上结果，Ollama模型很小，很快运行结束，但是不要尝试执行<code>ollama run xxx</code>，因为<code>run</code>命令是针对Chat模型的，这里是Embedding模型</p><p><strong>Ollama Pull 了Embedding模型就自动启用了，不需要任何其他命令加载</strong></p><p>我们可以通过<code>ollama list</code>查看到下载的模型</p><p><img src="http://cdn.tycoding.cn/docs/202502112056210.png" alt="image-20250211205634137" loading="lazy"></p><h3 id="_3-测试embedding模型" tabindex="-1">3. 测试Embedding模型 <a class="header-anchor" href="#_3-测试embedding模型" aria-label="Permalink to &quot;3. 测试Embedding模型&quot;">​</a></h3><p>执行如下脚本</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> http://localhost:11434/api/embeddings</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;model&quot;: &quot;nomic-embed-text&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;prompt&quot;: &quot;The sky is blue because of Rayleigh scattering&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}&#39;</span></span></code></pre></div><p><img src="http://cdn.tycoding.cn/docs/202502112058680.png" alt="image-20250211205831626" loading="lazy"></p><p>如上说明Embedding模型正在运行，并且模型名称是<code>nomic-embed-text</code>，访问地址是：<code>http://localhost:11434/api/embeddings</code></p><h3 id="_4-langchat配置embedding" tabindex="-1">4. LangChat配置Embedding <a class="header-anchor" href="#_4-langchat配置embedding" aria-label="Permalink to &quot;4. LangChat配置Embedding&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502112105200.png" alt="image-20250211210510117" loading="lazy"></p><ul><li>模型版本填 <code>nomic-embed-text</code> （Select中输入并回车即可）</li><li>BaseUrl填写：<code>http://localhost:11434/</code></li></ul><h3 id="_5-创建langchat知识库" tabindex="-1">5. 创建LangChat知识库 <a class="header-anchor" href="#_5-创建langchat知识库" aria-label="Permalink to &quot;5. 创建LangChat知识库&quot;">​</a></h3><p>知识库中只关联Embedding数据库和Embedding模型</p><p><img src="http://cdn.tycoding.cn/docs/202502112107089.png" alt="image-20250211210743993" loading="lazy"></p><h3 id="_6-导入知识库文档" tabindex="-1">6. 导入知识库文档 <a class="header-anchor" href="#_6-导入知识库文档" aria-label="Permalink to &quot;6. 导入知识库文档&quot;">​</a></h3><p>在上面配置好<code>nomic-embed-text</code>模型后，往知识库导入文档</p><p><img src="http://cdn.tycoding.cn/docs/202502112202699.png" alt="image-20250211220225536" loading="lazy"></p><p>如果你是按照上面步骤的Embedding模型，你应该会收到如下向量化失败错误：</p><p><img src="http://cdn.tycoding.cn/docs/202502112203163.png" alt="image-20250211220330043" loading="lazy"></p><h3 id="为什么报错error-expected-1024" tabindex="-1">为什么报错ERROR: expected 1024 <a class="header-anchor" href="#为什么报错error-expected-1024" aria-label="Permalink to &quot;为什么报错ERROR: expected 1024&quot;">​</a></h3><p>因为我们在LangChat配置的 <strong>1024向量纬度</strong> 的数据库，所以生成的表也只接收1024维度的数据。</p><p><strong>但是，</strong> <code>nomic-embed-text</code> 模型只能生成768维度的数据，并不是生成1024维度的数据（当然如果你使用公有云模型，他们的模型一般都能支持生成多维度的数据768、1024、1536等等）</p><p><strong>只不过我们下载的模型只支持生成单维度的向量数据。</strong></p><p><img src="http://cdn.tycoding.cn/docs/202502112211381.png" alt="image-20250211221101301" loading="lazy"></p><p>从上表中你可以查看到不同模型能生成什么维度的数据。</p><p>例如阿里云的文档中，关于Embedding模型的定义如下：（你在最初阶段就应该考虑哪种模型兼容哪种向量维度）</p><p><img src="http://cdn.tycoding.cn/docs/202502112251834.png" alt="image-20250211225113717" loading="lazy"></p><p><strong>遇到这种情况怎么处理？</strong></p><p>无论用的哪个模型，如果向量维度一旦不匹配，就必然会出现类似此报错信息。按照如下步骤开始解决此问题：（我们的前提是仍用本地的Embedding模型，当然你换一个能输出1024维度的Embedding模型也是可以的）</p><ol><li>使用如Navicat等客户端工具删除<code>langchat</code>库中的表</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502112254845.png" alt="image-20250211225400733" loading="lazy"></p><ol start="2"><li>在LangChat管理端修改 <strong>向量数据库的纬度配置</strong> ，如下修改为768维度</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502112303718.png" alt="image-20250211230358598" loading="lazy"></p><ol start="3"><li>重启LangChat后端项目，系统会重新生成此表，并接收768维度的向量</li></ol><p>正常情况，重启后，我们就可以重新导入文档进行向量化了。</p><h3 id="_7-重新导入知识库文档" tabindex="-1">7. 重新导入知识库文档 <a class="header-anchor" href="#_7-重新导入知识库文档" aria-label="Permalink to &quot;7. 重新导入知识库文档&quot;">​</a></h3><p>我们准备如下这个txt文档</p><p><img src="http://cdn.tycoding.cn/docs/202502112316282.png" alt="image-20250211231626120" loading="lazy"></p><p>正常情况，后端会提示向量化成功，会有如下日志：</p><p><img src="http://cdn.tycoding.cn/docs/202502112320265.png" alt="image-20250211232040168" loading="lazy"></p><p>在向量数据库中，能看到如下分段信息：</p><p><img src="http://cdn.tycoding.cn/docs/202502112319952.png" alt="image-20250211231945871" loading="lazy"></p><h3 id="_8-向量搜索测试" tabindex="-1">8. 向量搜索测试 <a class="header-anchor" href="#_8-向量搜索测试" aria-label="Permalink to &quot;8. 向量搜索测试&quot;">​</a></h3><p>正常情况，如果向量化成功，你能在LangChat页面看到如下切面信息：</p><p><img src="http://cdn.tycoding.cn/docs/202502112322040.png" alt="image-20250211232251955" loading="lazy"></p><p>那我们进行一下向量检索测试：</p><p><img src="http://cdn.tycoding.cn/docs/202502112323154.png" alt="image-20250211232340080" loading="lazy"></p><blockquote><p>到此为止，LangChat知识库配置已经结束</p></blockquote><h2 id="创建langchat-ai应用" tabindex="-1">创建LangChat AI应用 <a class="header-anchor" href="#创建langchat-ai应用" aria-label="Permalink to &quot;创建LangChat AI应用&quot;">​</a></h2><p>上面知识库配置成功后，下面开始创建LangChat AI应用</p><p><img src="http://cdn.tycoding.cn/docs/202502112325631.png" alt="image-20250211232536523" loading="lazy"></p><p>如下所示创建AI应用，这里进需要关联我们刚才设置的DeepSeek-R1模型</p><h3 id="配置langchat应用" tabindex="-1">配置LangChat应用 <a class="header-anchor" href="#配置langchat应用" aria-label="Permalink to &quot;配置LangChat应用&quot;">​</a></h3><p>创建LangChat应用后，关联刚才创建的知识库，即可进行知识库问答了</p><p><img src="http://cdn.tycoding.cn/docs/202502121128775.png" alt="image-20250212112844178" loading="lazy"></p><p>关联好我们创建的知识库后，直接测试就能引用知识库的内容了</p><h3 id="测试langchat应用" tabindex="-1">测试LangChat应用 <a class="header-anchor" href="#测试langchat应用" aria-label="Permalink to &quot;测试LangChat应用&quot;">​</a></h3><p><img src="http://cdn.tycoding.cn/docs/202502121131192.png" alt="image-20250212113128034" loading="lazy"></p><p>如上，说明了他刚才引用了我们上传的<code>langchat.txt</code>文档</p><h3 id="验证是否查询向量文本" tabindex="-1">验证是否查询向量文本？ <a class="header-anchor" href="#验证是否查询向量文本" aria-label="Permalink to &quot;验证是否查询向量文本？&quot;">​</a></h3><p><strong>验证此回答是否查询了知识库的向量信息？</strong></p><ol><li>我们可以在控制台看到如下打印日志：</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502121134946.png" alt="image-20250212113459881" loading="lazy"></p><p>后面的部分就是引用的知识库文档</p><ol start="2"><li>可以拿未配置知识库的普通聊天做测试</li></ol><p><img src="http://cdn.tycoding.cn/docs/202502121137702.png" alt="image-20250212113735608" loading="lazy"></p><p>可以看到未配置知识库，是不知道LangChat是什么的。</p>`,164)]))}const m=e(o,[["render",i]]);export{s as __pageData,m as default};
